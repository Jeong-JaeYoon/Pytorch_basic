{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch normalization이 미치는 영향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2947754c090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMQA\\anaconda3\\envs\\main\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "mnist_train = datasets.MNIST(root='MNIST_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='MNIST_data/', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=mnist_train, batch_size=32, shuffle=True, drop_last=True)\n",
    "train_test = DataLoader(dataset=mnist_test, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(basic_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32, bias=True)\n",
    "        self.fc2 = nn.Linear(32, 32, bias=True)\n",
    "        self.fc3 = nn.Linear(32, 10, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        print(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        print(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "class batchnorm_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(batchnorm_model,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32, bias=True)\n",
    "        self.fc2 = nn.Linear(32, 32, bias=True)\n",
    "        self.fc3 = nn.Linear(32, 10, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch1 = nn.BatchNorm1d(32)\n",
    "        self.batch2 = nn.BatchNorm1d(32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        print(out)\n",
    "        out = self.batch1(out)\n",
    "        print(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        print(out)\n",
    "        out = self.batch2(out)\n",
    "        print(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0985,  0.3523, -0.1310,  ..., -0.0348,  0.0980, -0.1088],\n",
      "        [-0.0687,  0.4167,  0.1739,  ...,  0.4393, -0.3086, -0.1690],\n",
      "        [ 0.1127,  0.1878, -0.1721,  ..., -0.0594, -0.2055,  0.1464],\n",
      "        ...,\n",
      "        [ 0.2664,  0.3396,  0.0141,  ..., -0.0230, -0.2620,  0.1863],\n",
      "        [-0.0839,  0.3810, -0.0706,  ...,  0.1994, -0.2350, -0.3610],\n",
      "        [ 0.2247,  0.0920, -0.0913,  ...,  0.1120, -0.1500, -0.0839]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0985, 0.3523, 0.0000,  ..., 0.0000, 0.0980, 0.0000],\n",
      "        [0.0000, 0.4167, 0.1739,  ..., 0.4393, 0.0000, 0.0000],\n",
      "        [0.1127, 0.1878, 0.0000,  ..., 0.0000, 0.0000, 0.1464],\n",
      "        ...,\n",
      "        [0.2664, 0.3396, 0.0141,  ..., 0.0000, 0.0000, 0.1863],\n",
      "        [0.0000, 0.3810, 0.0000,  ..., 0.1994, 0.0000, 0.0000],\n",
      "        [0.2247, 0.0920, 0.0000,  ..., 0.1120, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1531, -0.0840, -0.0233,  ...,  0.1201, -0.0629, -0.1779],\n",
      "        [-0.2691, -0.0878, -0.0110,  ...,  0.3167, -0.0952, -0.3387],\n",
      "        [-0.1963,  0.0110, -0.0645,  ...,  0.1892, -0.1253, -0.1683],\n",
      "        ...,\n",
      "        [-0.2159,  0.0167, -0.0092,  ...,  0.1894, -0.0441, -0.2615],\n",
      "        [-0.3845, -0.0720, -0.0449,  ...,  0.3677, -0.1175, -0.3924],\n",
      "        [-0.2485,  0.0093, -0.0497,  ...,  0.1416, -0.1681, -0.1583]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3167, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0110, 0.0000,  ..., 0.1892, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0167, 0.0000,  ..., 0.1894, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3677, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0093, 0.0000,  ..., 0.1416, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "base_model = basic_model()\n",
    "base_criterion = torch.nn.CrossEntropyLoss()\n",
    "base_optimizer = optim.Adam(base_model.parameters(), lr=0.001)\n",
    "\n",
    "base_train_loss = []\n",
    "base_test_loss = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    base_model.train()\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "\n",
    "        base_optimizer.zero_grad()\n",
    "        base_predict = base_model(X)\n",
    "        base_loss = base_criterion(base_predict, Y)\n",
    "        base_loss.backward()\n",
    "        base_optimizer.step()\n",
    "\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0755, -0.1436,  0.2441,  ...,  0.0120,  0.0356,  0.1408],\n",
      "        [-0.0237, -0.0166, -0.0407,  ...,  0.2503, -0.0739, -0.1568],\n",
      "        [-0.2325, -0.2433,  0.0261,  ...,  0.1156,  0.1225, -0.3519],\n",
      "        ...,\n",
      "        [-0.1136, -0.1849,  0.5610,  ..., -0.2251,  0.0367,  0.0197],\n",
      "        [ 0.2148, -0.1487, -0.1223,  ...,  0.1893,  0.0122, -0.1701],\n",
      "        [ 0.0588,  0.0563, -0.0128,  ...,  0.1446,  0.0223, -0.1234]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.9007,  0.1805,  1.1894,  ..., -0.0174, -0.8748,  1.9486],\n",
      "        [-0.5688,  1.1176, -0.5197,  ...,  1.4355, -1.5931, -0.0503],\n",
      "        [-1.9067, -0.5546, -0.1189,  ...,  0.6144, -0.3049, -1.3604],\n",
      "        ...,\n",
      "        [-1.1445, -0.1235,  3.0915,  ..., -1.4625, -0.8677,  1.1352],\n",
      "        [ 0.9595,  0.1430, -1.0095,  ...,  1.0637, -1.0279, -0.1392],\n",
      "        [-0.0398,  1.6550, -0.3523,  ...,  0.7911, -0.9618,  0.1741]],\n",
      "       grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([[0.0000, 0.1805, 1.1894,  ..., 0.0000, 0.0000, 1.9486],\n",
      "        [0.0000, 1.1176, 0.0000,  ..., 1.4355, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.6144, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 3.0915,  ..., 0.0000, 0.0000, 1.1352],\n",
      "        [0.9595, 0.1430, 0.0000,  ..., 1.0637, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6550, 0.0000,  ..., 0.7911, 0.0000, 0.1741]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 1.6494e-01,  6.3194e-02,  2.3606e-01,  ..., -3.0616e-02,\n",
      "          1.4847e-01, -6.6386e-02],\n",
      "        [ 2.5876e-01, -3.9485e-01,  1.1011e-01,  ..., -1.3813e-01,\n",
      "         -6.1504e-02, -2.2408e-01],\n",
      "        [-1.3142e-01,  1.6026e-01,  6.3391e-05,  ...,  7.9552e-01,\n",
      "          1.7715e-01, -5.0435e-01],\n",
      "        ...,\n",
      "        [ 4.6826e-01, -3.0868e-01,  2.2557e-01,  ...,  1.3947e-02,\n",
      "          5.6869e-01, -4.3694e-02],\n",
      "        [ 1.7736e-01, -5.1145e-01,  3.9863e-02,  ...,  1.6465e-01,\n",
      "          4.4201e-01,  4.1930e-02],\n",
      "        [ 2.5125e-01,  1.1320e-01,  1.3565e-01,  ...,  6.2092e-01,\n",
      "          3.0814e-02,  4.5814e-01]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.5840,  0.3744,  1.3911,  ..., -0.5955,  0.1512, -0.0610],\n",
      "        [-0.2805, -1.1006,  0.7315,  ..., -0.9095, -0.4250, -0.4488],\n",
      "        [-1.5426,  0.6870,  0.1552,  ...,  1.8172,  0.2299, -1.1379],\n",
      "        ...,\n",
      "        [ 0.3972, -0.8232,  1.3362,  ..., -0.4654,  1.3042, -0.0052],\n",
      "        [-0.5438, -1.4762,  0.3637,  ..., -0.0252,  0.9566,  0.2053],\n",
      "        [-0.3048,  0.5354,  0.8653,  ...,  1.3073, -0.1717,  1.2287]],\n",
      "       grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([[0.0000, 0.3744, 1.3911,  ..., 0.0000, 0.1512, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7315,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6870, 0.1552,  ..., 1.8172, 0.2299, 0.0000],\n",
      "        ...,\n",
      "        [0.3972, 0.0000, 1.3362,  ..., 0.0000, 1.3042, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3637,  ..., 0.0000, 0.9566, 0.2053],\n",
      "        [0.0000, 0.5354, 0.8653,  ..., 1.3073, 0.0000, 1.2287]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bn_model = batchnorm_model()\n",
    "bn_criterion = torch.nn.CrossEntropyLoss()\n",
    "bn_optimizer = optim.Adam(bn_model.parameters(), lr=0.001)\n",
    "\n",
    "bn_train_loss = []\n",
    "bn_test_loss = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    bn_model.train()\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "\n",
    "        bn_optimizer.zero_grad()\n",
    "        bn_predict = bn_model(X)\n",
    "        bn_loss = bn_criterion(bn_predict, Y)\n",
    "        bn_loss.backward()\n",
    "        bn_optimizer.step()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "bn_model = batchnorm_model()\n",
    "bn_criterion = torch.nn.CrossEntropyLoss()\n",
    "bn_optimizer = optim.Adam(bn_model.parameters(), lr=0.001)\n",
    "\n",
    "bn_train_loss = []\n",
    "bn_test_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5777bc8a7577125a1c00ed1671130ea029cae56addc813f4c39a4f837e26f28b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('main': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
