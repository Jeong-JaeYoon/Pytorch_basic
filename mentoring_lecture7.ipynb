{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout과 Dead Relu는 어떤 차이점이 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_linear1 = nn.Linear(4, 3, bias=True)\n",
    "relu_linear2 = nn.Linear(3, 3, bias=True)\n",
    "relu_linear3 = nn.Linear(3, 2, bias=True)\n",
    "\n",
    "relu_drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "relu_drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "relu_drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "class Relu_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_model, self).__init__()\n",
    "        self.fc1 = relu_linear1\n",
    "        self.fc2 = relu_linear2\n",
    "        self.fc3 = relu_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Relu_Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_Dropout_model, self).__init__()\n",
    "        self.fc1 = relu_drop_linear1\n",
    "        self.fc2 = relu_drop_linear2\n",
    "        self.fc3 = relu_drop_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dropout_model, self).__init__()\n",
    "        self.fc1 = drop_linear1\n",
    "        self.fc2 = drop_linear2\n",
    "        self.fc3 = drop_linear3\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []\n",
    "target = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.random.randint(0,10, size=4)\n",
    "    y = np.random.randint(0,2)\n",
    "    input.append(x)\n",
    "    target.append(y)\n",
    "\n",
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target)\n",
    "\n",
    "dataset = TensorDataset(input, target)\n",
    "loader = DataLoader(dataset = dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 2., 6., 9.]])\n",
      "tensor([1])\n",
      "tensor([[6., 0., 7., 2.]])\n",
      "tensor([0])\n",
      "tensor([[6., 2., 3., 6.]])\n",
      "tensor([1])\n",
      "tensor([[5., 5., 6., 1.]])\n",
      "tensor([1])\n",
      "tensor([[1., 2., 3., 7.]])\n",
      "tensor([0])\n",
      "tensor([[2., 3., 7., 3.]])\n",
      "tensor([0])\n",
      "tensor([[5., 1., 8., 2.]])\n",
      "tensor([1])\n",
      "tensor([[6., 6., 8., 0.]])\n",
      "tensor([1])\n",
      "tensor([[3., 1., 0., 0.]])\n",
      "tensor([1])\n",
      "tensor([[4., 0., 4., 5.]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1225,  0.4039, -0.1940,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1620,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5407, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4272,  0.0323, -0.1558],\n",
      "        [-0.1122, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1454,  0.3115])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1225,  0.4039, -0.1940,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1620,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5407, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4272,  0.0323, -0.1558],\n",
      "        [-0.1122, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1454,  0.3115])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1225,  0.4039, -0.1940,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1620,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5407, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4272,  0.0323, -0.1558],\n",
      "        [-0.1122, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1454,  0.3115])\n"
     ]
    }
   ],
   "source": [
    "relu_model = Relu_model()\n",
    "relu_dropout_model = Relu_Dropout_model()\n",
    "dropout_model = Dropout_model()\n",
    "\n",
    "print('\\n ReLU 모델 parameters \\n')\n",
    "for names in relu_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_model.state_dict()[names])\n",
    "    \n",
    "print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "for names in relu_dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_dropout_model.state_dict()[names])\n",
    "\n",
    "print('\\n Dropout 모델 parameters \\n')\n",
    "for names in dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(dropout_model.state_dict()[names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[1.6387, 0.0000, 2.1394]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1454,  0.3115]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1454,  0.3115]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.4905, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[3.2773, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.1430, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.7700, 0.0710]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, -0.0000, 4.2789]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0000, -0.0000, -1.0032]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0109, 0.2449]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1225,  0.4039, -0.1940,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1620,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5407, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4272,  0.0323, -0.1558],\n",
      "        [-0.1122, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1493,  0.3154])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0323, -0.1558],\n",
      "        [-0.0979, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1521,  0.3182])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1225,  0.4039, -0.1940,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0089,  0.2333, -0.0262,  0.3131]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2538])\n",
      "fc2.weight\n",
      "tensor([[ 0.1620,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2073]])\n",
      "fc2.bias\n",
      "tensor([0.5407, 0.0119, 0.4234])\n",
      "fc3.weight\n",
      "tensor([[ 0.4272,  0.0323, -0.1514],\n",
      "        [-0.1122, -0.0859,  0.0620]])\n",
      "fc3.bias\n",
      "tensor([-0.1499,  0.3159])\n",
      "\n",
      " 2\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[2.0724, 0.0000, 1.0518]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3719, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0095, 0.2737]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0095, 0.2737]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.5698, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 2.1035]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1521,  0.3182]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[4.1448, -0.0000, 1.9798]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.5248, -5.8047, -2.2428]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.2266, 0.6169]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1202,  0.4016, -0.1970,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0207,  0.2417, -0.0147,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4658,  0.1929, -0.2518])\n",
      "fc2.weight\n",
      "tensor([[ 0.1571,  0.5245, -0.4821],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5383, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4256,  0.0323, -0.1558],\n",
      "        [-0.1106, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1537,  0.3197])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0323, -0.1558],\n",
      "        [-0.0979, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1560,  0.3220])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1138,  0.3952, -0.2056,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0351,  0.2595,  0.0088,  0.3131]])\n",
      "fc1.bias\n",
      "tensor([ 0.4648,  0.1929, -0.2494])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.5245, -0.4882],\n",
      "        [-0.4797,  0.1231, -0.4779],\n",
      "        [-0.2665,  0.3361, -0.2039]])\n",
      "fc2.bias\n",
      "tensor([0.5363, 0.0110, 0.4251])\n",
      "fc3.weight\n",
      "tensor([[ 0.4251,  0.0557, -0.1424],\n",
      "        [-0.1101, -0.1094,  0.0530]])\n",
      "fc3.bias\n",
      "tensor([-0.1539,  0.3200])\n",
      "\n",
      " 3\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[1.0453, 0.0000, 2.4360]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1537,  0.3197]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1537,  0.3197]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.9576, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1560,  0.3220]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[0., -0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.0726, 0.0220, 0.8501]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.1822, 0.2445]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1202,  0.4016, -0.1970,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0207,  0.2417, -0.0147,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4658,  0.1929, -0.2518])\n",
      "fc2.weight\n",
      "tensor([[ 0.1571,  0.5245, -0.4821],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5383, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4256,  0.0323, -0.1558],\n",
      "        [-0.1106, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1475,  0.3136])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0323, -0.1558],\n",
      "        [-0.0979, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1498,  0.3159])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1138,  0.3952, -0.2056,  0.0353],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0351,  0.2595,  0.0088,  0.3131]])\n",
      "fc1.bias\n",
      "tensor([ 0.4648,  0.1929, -0.2494])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.5245, -0.4882],\n",
      "        [-0.4797,  0.1231, -0.4779],\n",
      "        [-0.2665,  0.3361, -0.2039]])\n",
      "fc2.bias\n",
      "tensor([0.5418, 0.0127, 0.4231])\n",
      "fc3.weight\n",
      "tensor([[ 0.4306,  0.0558, -0.1380],\n",
      "        [-0.1157, -0.1095,  0.0486]])\n",
      "fc3.bias\n",
      "tensor([-0.1487,  0.3148])\n",
      "\n",
      " 4\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.3350, 0.0000, 1.3636]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0356]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1530,  0.3159]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1530,  0.3159]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.9549, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.4003, 0.0000, 2.5991]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1498,  0.3159]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[  0.0000, -10.5833,   2.9840]], grad_fn=<MulBackward0>)\n",
      "tensor([[-12.9329,  -5.4314,  -7.4854]], grad_fn=<MulBackward0>)\n",
      "tensor([[-4.9880,  2.0417]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1217,  0.4016, -0.1955,  0.0372],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0219,  0.2417, -0.0135,  0.3197]])\n",
      "fc1.bias\n",
      "tensor([ 0.4662,  0.1929, -0.2515])\n",
      "fc2.weight\n",
      "tensor([[ 0.1571,  0.5245, -0.4821],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2741,  0.3361, -0.2176]])\n",
      "fc2.bias\n",
      "tensor([0.5383, 0.0119, 0.4200])\n",
      "fc3.weight\n",
      "tensor([[ 0.4256,  0.0323, -0.1556],\n",
      "        [-0.1106, -0.0859,  0.0662]])\n",
      "fc3.bias\n",
      "tensor([-0.1413,  0.3074])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0323, -0.1558],\n",
      "        [-0.0979, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1436,  0.3097])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1138,  0.3952, -0.2056,  0.0353],\n",
      "        [-0.4085, -0.2872, -0.3919, -0.3454],\n",
      "        [-0.0140,  0.2595, -0.0404,  0.2517]])\n",
      "fc1.bias\n",
      "tensor([ 0.4648,  0.2027, -0.2617])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4090, -0.4557],\n",
      "        [-0.4797,  0.0881, -0.4680],\n",
      "        [-0.2665,  0.3756, -0.2150]])\n",
      "fc2.bias\n",
      "tensor([0.5527, 0.0160, 0.4193])\n",
      "fc3.weight\n",
      "tensor([[ 0.3014,  0.0015, -0.2128],\n",
      "        [ 0.0136, -0.0552,  0.1234]])\n",
      "fc3.bias\n",
      "tensor([-0.1387,  0.3048])\n",
      "\n",
      " 5\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[1.9467, 0.0000, 1.3056]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2148, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.0499,  0.2837]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0499,  0.2837]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.5402, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[3.6352, 0.0000, 2.3366]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1436,  0.3097]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, -0.0000, 1.9499]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0000, -0.0000, 0.0001]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1388,  0.3048]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1199,  0.3999, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0273,  0.2471, -0.0070,  0.3208]])\n",
      "fc1.bias\n",
      "tensor([ 0.4659,  0.1929, -0.2504])\n",
      "fc2.weight\n",
      "tensor([[ 0.1528,  0.5245, -0.4850],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2741,  0.3361, -0.2176]])\n",
      "fc2.bias\n",
      "tensor([0.5361, 0.0119, 0.4200])\n",
      "fc3.weight\n",
      "tensor([[ 0.4247,  0.0323, -0.1556],\n",
      "        [-0.1097, -0.0859,  0.0662]])\n",
      "fc3.bias\n",
      "tensor([-0.1455,  0.3116])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0119, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0323, -0.1558],\n",
      "        [-0.0979, -0.0859,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1475,  0.3136])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1138,  0.3952, -0.2056,  0.0353],\n",
      "        [-0.4085, -0.2872, -0.3919, -0.3454],\n",
      "        [-0.0197,  0.2539, -0.0472,  0.2505]])\n",
      "fc1.bias\n",
      "tensor([ 0.4648,  0.2027, -0.2628])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4090, -0.4557],\n",
      "        [-0.4797,  0.0881, -0.4680],\n",
      "        [-0.2665,  0.3756, -0.2099]])\n",
      "fc2.bias\n",
      "tensor([0.5527, 0.0160, 0.4220])\n",
      "fc3.weight\n",
      "tensor([[ 0.3014,  0.0015, -0.2128],\n",
      "        [ 0.0136, -0.0552,  0.1234]])\n",
      "fc3.bias\n",
      "tensor([-0.1427,  0.3087])\n",
      "\n",
      " 6\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 0.5059]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2907, 0.0000, 0.3100]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.0703,  0.3002]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0703,  0.3002]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.8955, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0239, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1468,  0.3116]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.0000, -0.0000, -0.4205]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.4887, 0.4256, 1.0204]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0896, 0.4313]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1199,  0.3999, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0198,  0.2471, -0.0157,  0.3183]])\n",
      "fc1.bias\n",
      "tensor([ 0.4659,  0.1929, -0.2517])\n",
      "fc2.weight\n",
      "tensor([[ 0.1528,  0.5245, -0.4834],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2741,  0.3361, -0.2182]])\n",
      "fc2.bias\n",
      "tensor([0.5392, 0.0119, 0.4187])\n",
      "fc3.weight\n",
      "tensor([[ 0.4264,  0.0323, -0.1538],\n",
      "        [-0.1115, -0.0859,  0.0644]])\n",
      "fc3.bias\n",
      "tensor([-0.1396,  0.3057])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0134, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0324, -0.1558],\n",
      "        [-0.0979, -0.0861,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1414,  0.3075])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1138,  0.3952, -0.2056,  0.0353],\n",
      "        [-0.4085, -0.2872, -0.3919, -0.3454],\n",
      "        [-0.0319,  0.2539, -0.0615,  0.2465]])\n",
      "fc1.bias\n",
      "tensor([ 0.4648,  0.2027, -0.2648])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4090, -0.4571],\n",
      "        [-0.4797,  0.0881, -0.4683],\n",
      "        [-0.2665,  0.3756, -0.2082]])\n",
      "fc2.bias\n",
      "tensor([0.5561, 0.0166, 0.4180])\n",
      "fc3.weight\n",
      "tensor([[ 0.3101,  0.0040, -0.2068],\n",
      "        [ 0.0049, -0.0577,  0.1174]])\n",
      "fc3.bias\n",
      "tensor([-0.1368,  0.3029])\n",
      "\n",
      " 7\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[1.2506, 0.0000, 3.1515]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1396,  0.3057]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1396,  0.3057]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.4951, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[2.0144, 0.0000, 6.0743]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1414,  0.3075]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[2.2722, -0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0000, -2.1467, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1455,  0.4268]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1199,  0.3999, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0198,  0.2471, -0.0157,  0.3183]])\n",
      "fc1.bias\n",
      "tensor([ 0.4659,  0.1929, -0.2517])\n",
      "fc2.weight\n",
      "tensor([[ 0.1528,  0.5245, -0.4834],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2741,  0.3361, -0.2182]])\n",
      "fc2.bias\n",
      "tensor([0.5392, 0.0119, 0.4187])\n",
      "fc3.weight\n",
      "tensor([[ 0.4264,  0.0323, -0.1538],\n",
      "        [-0.1115, -0.0859,  0.0644]])\n",
      "fc3.bias\n",
      "tensor([-0.1435,  0.3096])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1085,  0.3992, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0140,  0.2350, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4639,  0.1929, -0.2529])\n",
      "fc2.weight\n",
      "tensor([[ 0.1384,  0.5245, -0.4796],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5335, 0.0134, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4129,  0.0324, -0.1558],\n",
      "        [-0.0979, -0.0861,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1453,  0.3114])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1168,  0.3961, -0.2030,  0.0392],\n",
      "        [-0.4085, -0.2872, -0.3919, -0.3454],\n",
      "        [-0.0319,  0.2539, -0.0615,  0.2465]])\n",
      "fc1.bias\n",
      "tensor([ 0.4652,  0.2027, -0.2648])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4090, -0.4571],\n",
      "        [-0.4807,  0.0881, -0.4683],\n",
      "        [-0.2665,  0.3756, -0.2082]])\n",
      "fc2.bias\n",
      "tensor([0.5561, 0.0162, 0.4180])\n",
      "fc3.weight\n",
      "tensor([[ 0.3101,  0.0118, -0.2068],\n",
      "        [ 0.0049, -0.0655,  0.1174]])\n",
      "fc3.bias\n",
      "tensor([-0.1404,  0.3065])\n",
      "\n",
      " 8\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[1.2255, 0.0000, 0.0549]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6999, 0.0000, 0.0708]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1440, 0.2361]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.1440, 0.2361]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.6482, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[2.3770, 0.0000, 0.0481]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.6785, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.5477, 0.1470]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[ 2.4233, -2.6200, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.3335, -2.7591, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.2763,  0.4855]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1179,  0.3992, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0229,  0.2482, -0.0157,  0.3183]])\n",
      "fc1.bias\n",
      "tensor([ 0.4652,  0.1929, -0.2506])\n",
      "fc2.weight\n",
      "tensor([[ 0.1496,  0.5245, -0.4836],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2729,  0.3361, -0.2182]])\n",
      "fc2.bias\n",
      "tensor([0.5367, 0.0119, 0.4197])\n",
      "fc3.weight\n",
      "tensor([[ 0.4231,  0.0323, -0.1541],\n",
      "        [-0.1081, -0.0859,  0.0647]])\n",
      "fc3.bias\n",
      "tensor([-0.1483,  0.3144])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1034,  0.3975, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0316,  0.2409, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4622,  0.1929, -0.2470])\n",
      "fc2.weight\n",
      "tensor([[ 0.1238,  0.5245, -0.4799],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5273, 0.0134, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4028,  0.0324, -0.1558],\n",
      "        [-0.0879, -0.0861,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1513,  0.3174])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1165,  0.3960, -0.2030,  0.0392],\n",
      "        [-0.4135, -0.2889, -0.3919, -0.3454],\n",
      "        [-0.0319,  0.2539, -0.0615,  0.2465]])\n",
      "fc1.bias\n",
      "tensor([ 0.4651,  0.2010, -0.2648])\n",
      "fc2.weight\n",
      "tensor([[ 0.1392,  0.4141, -0.4571],\n",
      "        [-0.4819,  0.0894, -0.4683],\n",
      "        [-0.2665,  0.3756, -0.2082]])\n",
      "fc2.bias\n",
      "tensor([0.5542, 0.0157, 0.4180])\n",
      "fc3.weight\n",
      "tensor([[ 0.3112,  0.0206, -0.2068],\n",
      "        [ 0.0038, -0.0742,  0.1174]])\n",
      "fc3.bias\n",
      "tensor([-0.1436,  0.3097])\n",
      "\n",
      " 9\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.6254, 0.0000, 1.3841]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.1483,  0.3144]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1483,  0.3144]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.9510, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[1.0368, 0.0000, 2.6557]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1513,  0.3174]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[1.1651, -0.0000, 1.4842]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0759, -2.4817, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1710,  0.4942]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1179,  0.3992, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0229,  0.2482, -0.0157,  0.3183]])\n",
      "fc1.bias\n",
      "tensor([ 0.4652,  0.1929, -0.2506])\n",
      "fc2.weight\n",
      "tensor([[ 0.1496,  0.5245, -0.4836],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2729,  0.3361, -0.2182]])\n",
      "fc2.bias\n",
      "tensor([0.5367, 0.0119, 0.4197])\n",
      "fc3.weight\n",
      "tensor([[ 0.4231,  0.0323, -0.1541],\n",
      "        [-0.1081, -0.0859,  0.0647]])\n",
      "fc3.bias\n",
      "tensor([-0.1421,  0.3082])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1034,  0.3975, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0316,  0.2409, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4622,  0.1929, -0.2470])\n",
      "fc2.weight\n",
      "tensor([[ 0.1238,  0.5245, -0.4799],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5273, 0.0134, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4028,  0.0324, -0.1558],\n",
      "        [-0.0879, -0.0861,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1451,  0.3112])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1164,  0.3958, -0.2035,  0.0389],\n",
      "        [-0.4135, -0.2889, -0.3919, -0.3454],\n",
      "        [-0.0417,  0.2392, -0.0956,  0.2318]])\n",
      "fc1.bias\n",
      "tensor([ 0.4650,  0.2010, -0.2697])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4141, -0.4511],\n",
      "        [-0.4805,  0.0894, -0.4665],\n",
      "        [-0.2665,  0.3756, -0.2082]])\n",
      "fc2.bias\n",
      "tensor([0.5582, 0.0170, 0.4180])\n",
      "fc3.weight\n",
      "tensor([[ 0.3117,  0.0042, -0.2068],\n",
      "        [ 0.0033, -0.0578,  0.1174]])\n",
      "fc3.bias\n",
      "tensor([-0.1370,  0.3031])\n",
      "\n",
      " 10\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 0.6224]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2357, 0.0000, 0.2840]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.0862,  0.3011]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0862,  0.3011]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.5181, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 1.1980]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1451,  0.3112]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.2157, -0.0000, -1.0811]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 1.2498, 1.4013]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.4216,  0.3953]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1179,  0.3992, -0.1977,  0.0368],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0271,  0.2490, -0.0090,  0.3200]])\n",
      "fc1.bias\n",
      "tensor([ 0.4652,  0.1929, -0.2498])\n",
      "fc2.weight\n",
      "tensor([[ 0.1496,  0.5245, -0.4849],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2729,  0.3361, -0.2176]])\n",
      "fc2.bias\n",
      "tensor([0.5345, 0.0119, 0.4206])\n",
      "fc3.weight\n",
      "tensor([[ 0.4221,  0.0323, -0.1553],\n",
      "        [-0.1072, -0.0859,  0.0659]])\n",
      "fc3.bias\n",
      "tensor([-0.1462,  0.3123])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1034,  0.3975, -0.2010,  0.0213],\n",
      "        [-0.4475, -0.2872, -0.4309, -0.3942],\n",
      "        [ 0.0316,  0.2409, -0.0237,  0.3182]])\n",
      "fc1.bias\n",
      "tensor([ 0.4622,  0.1929, -0.2470])\n",
      "fc2.weight\n",
      "tensor([[ 0.1238,  0.5245, -0.4799],\n",
      "        [-0.4758,  0.1231, -0.4760],\n",
      "        [-0.2737,  0.3361, -0.2157]])\n",
      "fc2.bias\n",
      "tensor([0.5273, 0.0134, 0.4214])\n",
      "fc3.weight\n",
      "tensor([[ 0.4028,  0.0324, -0.1558],\n",
      "        [-0.0879, -0.0861,  0.0664]])\n",
      "fc3.bias\n",
      "tensor([-0.1490,  0.3151])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[ 0.1129,  0.3951, -0.2091,  0.0375],\n",
      "        [-0.4135, -0.2889, -0.3919, -0.3454],\n",
      "        [-0.0441,  0.2388, -0.0994,  0.2309]])\n",
      "fc1.bias\n",
      "tensor([ 0.4643,  0.2010, -0.2702])\n",
      "fc2.weight\n",
      "tensor([[ 0.1439,  0.4141, -0.4511],\n",
      "        [-0.4804,  0.0894, -0.4660],\n",
      "        [-0.2670,  0.3756, -0.2104]])\n",
      "fc2.bias\n",
      "tensor([0.5582, 0.0166, 0.4200])\n",
      "fc3.weight\n",
      "tensor([[ 0.3117,  0.0003, -0.2111],\n",
      "        [ 0.0033, -0.0540,  0.1217]])\n",
      "fc3.bias\n",
      "tensor([-0.1401,  0.3061])\n"
     ]
    }
   ],
   "source": [
    "relu_optimizer = optim.SGD(relu_model.parameters(), lr = 0.01)\n",
    "relu_dropout_optimizer = optim.SGD(relu_dropout_model.parameters(), lr=0.01)\n",
    "dropout_optimizer = optim.SGD(dropout_model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i, data in enumerate(loader): \n",
    "    relu_model.train()\n",
    "    relu_dropout_model.train()\n",
    "    dropout_model.train()    \n",
    "    \n",
    "    print('\\n', i+1)\n",
    "    x,y = data\n",
    "    \n",
    "    relu_optimizer.zero_grad()\n",
    "    relu_dropout_optimizer.zero_grad()\n",
    "    dropout_optimizer.zero_grad()       \n",
    "    \n",
    "    print('ReLU 모델 output \\n')\n",
    "    r_output = relu_model(x)\n",
    "    print(r_output)\n",
    "    r_loss = criterion(r_output,y)\n",
    "    print(r_loss)\n",
    "    r_loss.backward()\n",
    "    relu_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU + Dropout 모델 output \\n')\n",
    "    rd_output = relu_dropout_model(x)\n",
    "    rd_loss = criterion(rd_output, y)\n",
    "    rd_loss.backward()\n",
    "    relu_dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n Dropout 모델 output \\n')\n",
    "    d_output = dropout_model(x)\n",
    "    d_loss = criterion(d_output, y)\n",
    "    d_loss.backward()\n",
    "    dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU 모델 parameters \\n')\n",
    "    for names in relu_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_model.state_dict()[names])\n",
    "        \n",
    "    print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "    for names in relu_dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_dropout_model.state_dict()[names])\n",
    "    \n",
    "    print('\\n Dropout 모델 parameters \\n')\n",
    "    for names in dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(dropout_model.state_dict()[names])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2739e+00,  4.7268e-01,  1.9138e-01, -2.0238e-01, -2.4849e-01],\n",
      "        [ 5.9721e-01,  2.5011e-01, -7.2537e-01, -1.1257e+00,  4.3374e-01],\n",
      "        [-2.3343e-04, -4.6638e-02,  2.1673e+00, -2.7834e-01,  9.6699e-01]],\n",
      "       requires_grad=True)\n",
      "tensor([1, 4, 0])\n",
      "tensor(1.6601, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5361170131513125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.log(0.9119) + np.log(1.8745))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5777bc8a7577125a1c00ed1671130ea029cae56addc813f4c39a4f837e26f28b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('main': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
