{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout과 Dead Relu는 어떤 차이점이 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_linear1 = nn.Linear(4, 3, bias=True)\n",
    "relu_linear2 = nn.Linear(3, 3, bias=True)\n",
    "relu_linear3 = nn.Linear(3, 2, bias=True)\n",
    "\n",
    "relu_drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "relu_drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "relu_drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "class Relu_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_model, self).__init__()\n",
    "        self.fc1 = relu_linear1\n",
    "        self.fc2 = relu_linear2\n",
    "        self.fc3 = relu_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Relu_Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_Dropout_model, self).__init__()\n",
    "        self.fc1 = relu_drop_linear1\n",
    "        self.fc2 = relu_drop_linear2\n",
    "        self.fc3 = relu_drop_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dropout_model, self).__init__()\n",
    "        self.fc1 = drop_linear1\n",
    "        self.fc2 = drop_linear2\n",
    "        self.fc3 = drop_linear3\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []\n",
    "target = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.random.randint(0,10, size=4)\n",
    "    y = np.random.randint(0,2)\n",
    "    input.append(x)\n",
    "    target.append(y)\n",
    "\n",
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target)\n",
    "\n",
    "dataset = TensorDataset(input, target)\n",
    "loader = DataLoader(dataset = dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1., 4., 8.]])\n",
      "tensor([1])\n",
      "tensor([[2., 4., 8., 6.]])\n",
      "tensor([1])\n",
      "tensor([[7., 8., 0., 4.]])\n",
      "tensor([0])\n",
      "tensor([[5., 0., 6., 9.]])\n",
      "tensor([1])\n",
      "tensor([[8., 5., 7., 5.]])\n",
      "tensor([1])\n",
      "tensor([[7., 3., 2., 7.]])\n",
      "tensor([1])\n",
      "tensor([[1., 4., 3., 1.]])\n",
      "tensor([1])\n",
      "tensor([[4., 4., 0., 6.]])\n",
      "tensor([1])\n",
      "tensor([[0., 7., 2., 8.]])\n",
      "tensor([0])\n",
      "tensor([[6., 5., 7., 5.]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3862])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3422],\n",
      "        [ 0.3710,  0.1598,  0.2137]])\n",
      "fc3.bias\n",
      "tensor([-0.5740,  0.4736])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3862])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3422],\n",
      "        [ 0.3710,  0.1598,  0.2137]])\n",
      "fc3.bias\n",
      "tensor([-0.5740,  0.4736])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3862])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3422],\n",
      "        [ 0.3710,  0.1598,  0.2137]])\n",
      "fc3.bias\n",
      "tensor([-0.5740,  0.4736])\n"
     ]
    }
   ],
   "source": [
    "relu_model = Relu_model()\n",
    "relu_dropout_model = Relu_Dropout_model()\n",
    "dropout_model = Dropout_model()\n",
    "\n",
    "print('\\n ReLU 모델 parameters \\n')\n",
    "for names in relu_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_model.state_dict()[names])\n",
    "    \n",
    "print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "for names in relu_dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_dropout_model.state_dict()[names])\n",
    "\n",
    "print('\\n Dropout 모델 parameters \\n')\n",
    "for names in dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(dropout_model.state_dict()[names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.8495, 2.8050, 0.6089]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.2109, 0.0699]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7009,  0.5222]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 5.6100, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5740,  0.4736]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[1.6990, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0000, 0.0000, 1.7298]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.1659,  0.8433]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3028, -0.3836,  0.0472,  0.2412],\n",
      "        [-0.0626, -0.3512, -0.1631,  0.4487],\n",
      "        [ 0.2489, -0.4572,  0.0752, -0.0946]])\n",
      "fc1.bias\n",
      "tensor([0.5060, 0.2894, 0.3467])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1887, -0.0613,  0.1052],\n",
      "        [ 0.2917, -0.1552, -0.1413]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2110,  0.3962])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4984, -0.3522],\n",
      "        [ 0.3710,  0.1698,  0.2237]])\n",
      "fc3.bias\n",
      "tensor([-0.5840,  0.4836])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3862])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3422],\n",
      "        [ 0.3710,  0.1598,  0.2137]])\n",
      "fc3.bias\n",
      "tensor([-0.5840,  0.4836])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3028, -0.3836,  0.0472,  0.2412],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.5060, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2917, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3962])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3522],\n",
      "        [ 0.3710,  0.1598,  0.2237]])\n",
      "fc3.bias\n",
      "tensor([-0.5840,  0.4836])\n",
      "\n",
      " 2\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.0946, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1440, 0.2263]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7355,  0.5587]], grad_fn=<AddmmBackward>)\n",
      "tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 2.5491, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5840,  0.4836]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.3103,  0.0000, -6.5607]], grad_fn=<MulBackward0>)\n",
      "tensor([[-6.5977, -0.9578,  2.5969]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.6483, -1.5365]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2961, -0.3769,  0.0539,  0.2479],\n",
      "        [-0.0693, -0.3440, -0.1603,  0.4539],\n",
      "        [ 0.2422, -0.4640,  0.0685, -0.1013]])\n",
      "fc1.bias\n",
      "tensor([0.5127, 0.2945, 0.3400])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1954, -0.0634,  0.1119],\n",
      "        [ 0.2984, -0.1574, -0.1346]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2057,  0.3909])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4942, -0.3453],\n",
      "        [ 0.3710,  0.1655,  0.2169]])\n",
      "fc3.bias\n",
      "tensor([-0.5787,  0.4783])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3862])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3422],\n",
      "        [ 0.3710,  0.1598,  0.2137]])\n",
      "fc3.bias\n",
      "tensor([-0.5792,  0.4788])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2961, -0.3902,  0.0489,  0.2395],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4547,  0.0777, -0.0920]])\n",
      "fc1.bias\n",
      "tensor([0.5043, 0.2994, 0.3492])\n",
      "fc2.weight\n",
      "tensor([[-0.4677,  0.0661,  0.4604],\n",
      "        [ 0.1862, -0.0713,  0.1026],\n",
      "        [ 0.3010, -0.1652, -0.1439]])\n",
      "fc2.bias\n",
      "tensor([-0.4817,  0.1936,  0.3917])\n",
      "fc3.weight\n",
      "tensor([[-0.0654, -0.4959, -0.3467],\n",
      "        [ 0.3785,  0.1672,  0.2183]])\n",
      "fc3.bias\n",
      "tensor([-0.5796,  0.4792])\n",
      "\n",
      " 3\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.2057, 0.3909]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.8154,  0.5971]], grad_fn=<AddmmBackward>)\n",
      "tensor(1.6305, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.7725]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.8435,  0.6439]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-3.3667, -0.0000, -0.5739]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., -0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5796,  0.4792]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2909, -0.3717,  0.0591,  0.2531],\n",
      "        [-0.0745, -0.3385, -0.1581,  0.4578],\n",
      "        [ 0.2370, -0.4691,  0.0633, -0.1065]])\n",
      "fc1.bias\n",
      "tensor([0.5179, 0.2985, 0.3348])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2006, -0.0651,  0.1171],\n",
      "        [ 0.3036, -0.1590, -0.1294]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1983,  0.3834])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4871, -0.3372],\n",
      "        [ 0.3710,  0.1585,  0.2088]])\n",
      "fc3.bias\n",
      "tensor([-0.5713,  0.4709])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2010,  0.3798])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4884, -0.3358],\n",
      "        [ 0.3710,  0.1598,  0.2074]])\n",
      "fc3.bias\n",
      "tensor([-0.5720,  0.4716])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2909, -0.3953,  0.0502,  0.2382],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4604,  0.0720, -0.0978]])\n",
      "fc1.bias\n",
      "tensor([0.5030, 0.2994, 0.3435])\n",
      "fc2.weight\n",
      "tensor([[-0.4620,  0.0661,  0.4662],\n",
      "        [ 0.1919, -0.0713,  0.1084],\n",
      "        [ 0.3081, -0.1652, -0.1381]])\n",
      "fc2.bias\n",
      "tensor([-0.4875,  0.1879,  0.3883])\n",
      "fc3.weight\n",
      "tensor([[-0.0712, -0.5016, -0.3425],\n",
      "        [ 0.3842,  0.1730,  0.2141]])\n",
      "fc3.bias\n",
      "tensor([-0.5724,  0.4720])\n",
      "\n",
      " 4\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.4405, 0.2779, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.2686, 0.4730]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.8616,  0.6122]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.4021, 0.7597]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.0235,  0.6933]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.9749,  0.0000,  0.7765]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.7690,  0.2637]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2847, -0.3650,  0.0663,  0.2598],\n",
      "        [-0.0805, -0.3348, -0.1603,  0.4601],\n",
      "        [ 0.2327, -0.4734,  0.0591, -0.1107]])\n",
      "fc1.bias\n",
      "tensor([0.5249, 0.3004, 0.3305])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2068, -0.0661,  0.1213],\n",
      "        [ 0.3098, -0.1601, -0.1252]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1932,  0.3783])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4829, -0.3321],\n",
      "        [ 0.3710,  0.1542,  0.2037]])\n",
      "fc3.bias\n",
      "tensor([-0.5663,  0.4659])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2069,  0.3757])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4942, -0.3317],\n",
      "        [ 0.3710,  0.1656,  0.2033]])\n",
      "fc3.bias\n",
      "tensor([-0.5669,  0.4665])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2867, -0.3996,  0.0513,  0.2372],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4652,  0.0673, -0.1025]])\n",
      "fc1.bias\n",
      "tensor([0.5020, 0.2994, 0.3388])\n",
      "fc2.weight\n",
      "tensor([[-0.4573,  0.0661,  0.4709],\n",
      "        [ 0.1967, -0.0713,  0.1131],\n",
      "        [ 0.3139, -0.1652, -0.1334]])\n",
      "fc2.bias\n",
      "tensor([-0.4868,  0.1831,  0.3897])\n",
      "fc3.weight\n",
      "tensor([[-0.0751, -0.5063, -0.3406],\n",
      "        [ 0.3881,  0.1777,  0.2122]])\n",
      "fc3.bias\n",
      "tensor([-0.5686,  0.4682])\n",
      "\n",
      " 5\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1932, 0.3783]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7852,  0.5727]], grad_fn=<AddmmBackward>)\n",
      "tensor(1.5868, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5669,  0.4665]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-7.5051, -1.9272, -3.9608]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0000, -3.2066, -2.2389]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 1.8177, -0.5767]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2794, -0.3593,  0.0723,  0.2655],\n",
      "        [-0.0856, -0.3317, -0.1622,  0.4620],\n",
      "        [ 0.2292, -0.4770,  0.0555, -0.1143]])\n",
      "fc1.bias\n",
      "tensor([0.5308, 0.3020, 0.3270])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2120, -0.0670,  0.1249],\n",
      "        [ 0.3150, -0.1611, -0.1216]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1866,  0.3718])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4767, -0.3253],\n",
      "        [ 0.3710,  0.1481,  0.1969]])\n",
      "fc3.bias\n",
      "tensor([-0.5597,  0.4593])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2118,  0.3723])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4992, -0.3282],\n",
      "        [ 0.3710,  0.1705,  0.1998]])\n",
      "fc3.bias\n",
      "tensor([-0.5604,  0.4600])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2904, -0.4060,  0.0522,  0.2342],\n",
      "        [-0.0472, -0.3357, -0.1531,  0.4642],\n",
      "        [ 0.2534, -0.4692,  0.0633, -0.1065]])\n",
      "fc1.bias\n",
      "tensor([0.4977, 0.3048, 0.3348])\n",
      "fc2.weight\n",
      "tensor([[-0.4533,  0.0661,  0.4749],\n",
      "        [ 0.2026, -0.0658,  0.1180],\n",
      "        [ 0.3207, -0.1598, -0.1286]])\n",
      "fc2.bias\n",
      "tensor([-0.4863,  0.1777,  0.3899])\n",
      "fc3.weight\n",
      "tensor([[-0.0784, -0.5130, -0.3403],\n",
      "        [ 0.3914,  0.1843,  0.2119]])\n",
      "fc3.bias\n",
      "tensor([-0.5649,  0.4645])\n",
      "\n",
      " 6\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1866, 0.3718]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7696,  0.5601]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2347, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 0.7294]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5604,  0.4600]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-4.6377, -0.0000, -0.1464]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0000, -0.0000, -2.1573]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.1693, 0.0073]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2749, -0.3543,  0.0775,  0.2704],\n",
      "        [-0.0900, -0.3290, -0.1638,  0.4636],\n",
      "        [ 0.2261, -0.4800,  0.0524, -0.1174]])\n",
      "fc1.bias\n",
      "tensor([0.5360, 0.3034, 0.3239])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2165, -0.0678,  0.1280],\n",
      "        [ 0.3195, -0.1618, -0.1185]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1817,  0.3669])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4723, -0.3203],\n",
      "        [ 0.3710,  0.1436,  0.1919]])\n",
      "fc3.bias\n",
      "tensor([-0.5548,  0.4544])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2160,  0.3693])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.5034, -0.3252],\n",
      "        [ 0.3710,  0.1747,  0.1968]])\n",
      "fc3.bias\n",
      "tensor([-0.5559,  0.4554])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2861, -0.4042,  0.0575,  0.2376],\n",
      "        [-0.0425, -0.3310, -0.1531,  0.4689],\n",
      "        [ 0.2482, -0.4747,  0.0572, -0.1119]])\n",
      "fc1.bias\n",
      "tensor([0.5012, 0.3095, 0.3288])\n",
      "fc2.weight\n",
      "tensor([[-0.4499,  0.0661,  0.4783],\n",
      "        [ 0.2077, -0.0611,  0.1222],\n",
      "        [ 0.3172, -0.1551, -0.1246]])\n",
      "fc2.bias\n",
      "tensor([-0.4859,  0.1731,  0.3940])\n",
      "fc3.weight\n",
      "tensor([[-0.0812, -0.5187, -0.3359],\n",
      "        [ 0.3943,  0.1901,  0.2075]])\n",
      "fc3.bias\n",
      "tensor([-0.5650,  0.4645])\n",
      "\n",
      " 7\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.4091, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0861, 0.1388]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.6400,  0.4934]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 2.9529, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5559,  0.4554]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-1.6689,  3.2576, -2.4976]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0., -0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5650,  0.4645]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2709, -0.3501,  0.0820,  0.2747],\n",
      "        [-0.0963, -0.3275, -0.1652,  0.4640],\n",
      "        [ 0.2234, -0.4827,  0.0497, -0.1201]])\n",
      "fc1.bias\n",
      "tensor([0.5404, 0.3032, 0.3212])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2204, -0.0670,  0.1307],\n",
      "        [ 0.3234, -0.1611, -0.1158]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1783,  0.3634])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4688, -0.3164],\n",
      "        [ 0.3710,  0.1402,  0.1880]])\n",
      "fc3.bias\n",
      "tensor([-0.5515,  0.4511])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3128, -0.3936,  0.0372,  0.2312],\n",
      "        [-0.0526, -0.3412, -0.1531,  0.4587],\n",
      "        [ 0.2589, -0.4472,  0.0852, -0.0846]])\n",
      "fc1.bias\n",
      "tensor([0.4960, 0.2994, 0.3567])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.1787, -0.0713,  0.0952],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.2197,  0.3667])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.5071, -0.3226],\n",
      "        [ 0.3710,  0.1784,  0.1942]])\n",
      "fc3.bias\n",
      "tensor([-0.5529,  0.4525])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2824, -0.4025,  0.0621,  0.2405],\n",
      "        [-0.0384, -0.3269, -0.1531,  0.4730],\n",
      "        [ 0.2436, -0.4796,  0.0519, -0.1166]])\n",
      "fc1.bias\n",
      "tensor([0.5042, 0.3136, 0.3236])\n",
      "fc2.weight\n",
      "tensor([[-0.4469,  0.0661,  0.4813],\n",
      "        [ 0.2122, -0.0570,  0.1259],\n",
      "        [ 0.3141, -0.1510, -0.1212]])\n",
      "fc2.bias\n",
      "tensor([-0.4855,  0.1690,  0.3977])\n",
      "fc3.weight\n",
      "tensor([[-0.0837, -0.5237, -0.3321],\n",
      "        [ 0.3968,  0.1951,  0.2037]])\n",
      "fc3.bias\n",
      "tensor([-0.5663,  0.4659])\n",
      "\n",
      " 8\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[2.1508, 3.0063, 0.6554]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.5366, 0.4987]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.9609,  0.6200]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.1871, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[2.4718, 6.4921, 2.8017]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0985, 0.9308, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.0306,  0.6551]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0000, 0.3381, 0.7954]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.0075,  0.6938]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2650, -0.3463,  0.0880,  0.2808],\n",
      "        [-0.1035, -0.3262, -0.1683,  0.4633],\n",
      "        [ 0.2249, -0.4851,  0.0517, -0.1188]])\n",
      "fc1.bias\n",
      "tensor([0.5464, 0.3022, 0.3221])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2265, -0.0644,  0.1356],\n",
      "        [ 0.3294, -0.1586, -0.1110]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1758,  0.3609])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4676, -0.3138],\n",
      "        [ 0.3710,  0.1389,  0.1854]])\n",
      "fc3.bias\n",
      "tensor([-0.5492,  0.4487])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3178, -0.3936,  0.0323,  0.2262],\n",
      "        [-0.0576, -0.3412, -0.1581,  0.4538],\n",
      "        [ 0.2638, -0.4472,  0.0901, -0.0796]])\n",
      "fc1.bias\n",
      "tensor([0.4910, 0.2944, 0.3616])\n",
      "fc2.weight\n",
      "tensor([[-0.4702,  0.0710,  0.4580],\n",
      "        [ 0.1837, -0.0663,  0.1001],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4693,  0.2256,  0.3644])\n",
      "fc3.weight\n",
      "tensor([[-0.0629, -0.5129, -0.3203],\n",
      "        [ 0.3760,  0.1843,  0.1919]])\n",
      "fc3.bias\n",
      "tensor([-0.5509,  0.4505])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2791, -0.4011,  0.0662,  0.2431],\n",
      "        [-0.0347, -0.3233, -0.1531,  0.4766],\n",
      "        [ 0.2395, -0.4839,  0.0472, -0.1207]])\n",
      "fc1.bias\n",
      "tensor([0.5068, 0.3172, 0.3190])\n",
      "fc2.weight\n",
      "tensor([[-0.4442,  0.0661,  0.4840],\n",
      "        [ 0.2161, -0.0534,  0.1291],\n",
      "        [ 0.3113, -0.1474, -0.1181]])\n",
      "fc2.bias\n",
      "tensor([-0.4851,  0.1684,  0.4019])\n",
      "fc3.weight\n",
      "tensor([[-0.0859, -0.5288, -0.3291],\n",
      "        [ 0.3990,  0.2001,  0.2007]])\n",
      "fc3.bias\n",
      "tensor([-0.5682,  0.4678])\n",
      "\n",
      " 9\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.5053, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0788, 0.1222]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.6244,  0.4823]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2857, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5509,  0.4505]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-1.6317,  0.0000, -0.4133]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0000, -0.0000, -0.1146]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.5305,  0.4448]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2597, -0.3429,  0.0934,  0.2861],\n",
      "        [-0.1115, -0.3256, -0.1718,  0.4616],\n",
      "        [ 0.2262, -0.4872,  0.0534, -0.1177]])\n",
      "fc1.bias\n",
      "tensor([0.5517, 0.3000, 0.3229])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2318, -0.0609,  0.1399],\n",
      "        [ 0.3348, -0.1552, -0.1067]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1744,  0.3595])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4668, -0.3118],\n",
      "        [ 0.3710,  0.1381,  0.1834]])\n",
      "fc3.bias\n",
      "tensor([-0.5480,  0.4475])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3222, -0.3936,  0.0279,  0.2218],\n",
      "        [-0.0620, -0.3412, -0.1625,  0.4494],\n",
      "        [ 0.2682, -0.4472,  0.0945, -0.0752]])\n",
      "fc1.bias\n",
      "tensor([0.4866, 0.2900, 0.3660])\n",
      "fc2.weight\n",
      "tensor([[-0.4658,  0.0754,  0.4624],\n",
      "        [ 0.1881, -0.0619,  0.1045],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4649,  0.2308,  0.3623])\n",
      "fc3.weight\n",
      "tensor([[-0.0673, -0.5181, -0.3183],\n",
      "        [ 0.3804,  0.1894,  0.1899]])\n",
      "fc3.bias\n",
      "tensor([-0.5500,  0.4496])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2745, -0.3986,  0.0704,  0.2475],\n",
      "        [-0.0315, -0.3201, -0.1531,  0.4798],\n",
      "        [ 0.2344, -0.4883,  0.0425, -0.1256]])\n",
      "fc1.bias\n",
      "tensor([0.5109, 0.3204, 0.3138])\n",
      "fc2.weight\n",
      "tensor([[-0.4419,  0.0661,  0.4863],\n",
      "        [ 0.2196, -0.0502,  0.1320],\n",
      "        [ 0.3082, -0.1441, -0.1157]])\n",
      "fc2.bias\n",
      "tensor([-0.4848,  0.1677,  0.4071])\n",
      "fc3.weight\n",
      "tensor([[-0.0878, -0.5332, -0.3264],\n",
      "        [ 0.4009,  0.2046,  0.1980]])\n",
      "fc3.bias\n",
      "tensor([-0.5711,  0.4706])\n",
      "\n",
      " 10\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.1744, 0.3595]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7415,  0.5376]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.7246]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.7807,  0.5872]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.0000, -1.9419, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.2263,  0.5304,  1.3739]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.1947,  0.3596]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2550, -0.3399,  0.0981,  0.2909],\n",
      "        [-0.1187, -0.3251, -0.1749,  0.4600],\n",
      "        [ 0.2274, -0.4891,  0.0550, -0.1168]])\n",
      "fc1.bias\n",
      "tensor([0.5564, 0.2982, 0.3237])\n",
      "fc2.weight\n",
      "tensor([[-0.4752,  0.0661,  0.4530],\n",
      "        [ 0.2366, -0.0577,  0.1438],\n",
      "        [ 0.3395, -0.1522, -0.1028]])\n",
      "fc2.bias\n",
      "tensor([-0.4743,  0.1739,  0.3589])\n",
      "fc3.weight\n",
      "tensor([[-0.0580, -0.4667, -0.3109],\n",
      "        [ 0.3710,  0.1381,  0.1825]])\n",
      "fc3.bias\n",
      "tensor([-0.5476,  0.4472])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.3261, -0.3936,  0.0239,  0.2179],\n",
      "        [-0.0659, -0.3412, -0.1664,  0.4454],\n",
      "        [ 0.2721, -0.4472,  0.0985, -0.0713]])\n",
      "fc1.bias\n",
      "tensor([0.4827, 0.2861, 0.3700])\n",
      "fc2.weight\n",
      "tensor([[-0.4619,  0.0794,  0.4663],\n",
      "        [ 0.1920, -0.0580,  0.1085],\n",
      "        [ 0.2817, -0.1652, -0.1513]])\n",
      "fc2.bias\n",
      "tensor([-0.4610,  0.2354,  0.3616])\n",
      "fc3.weight\n",
      "tensor([[-0.0713, -0.5227, -0.3176],\n",
      "        [ 0.3843,  0.1941,  0.1892]])\n",
      "fc3.bias\n",
      "tensor([-0.5499,  0.4495])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.2705, -0.3965,  0.0742,  0.2515],\n",
      "        [-0.0295, -0.3201, -0.1580,  0.4812],\n",
      "        [ 0.2298, -0.4922,  0.0383, -0.1299]])\n",
      "fc1.bias\n",
      "tensor([0.5146, 0.3186, 0.3092])\n",
      "fc2.weight\n",
      "tensor([[-0.4398,  0.0612,  0.4884],\n",
      "        [ 0.2228, -0.0535,  0.1346],\n",
      "        [ 0.3053, -0.1472, -0.1135]])\n",
      "fc2.bias\n",
      "tensor([-0.4824,  0.1697,  0.4126])\n",
      "fc3.weight\n",
      "tensor([[-0.0890, -0.5382, -0.3248],\n",
      "        [ 0.4021,  0.2096,  0.1964]])\n",
      "fc3.bias\n",
      "tensor([-0.5744,  0.4739])\n"
     ]
    }
   ],
   "source": [
    "relu_optimizer = optim.Adam(relu_model.parameters(), lr = 0.01)\n",
    "relu_dropout_optimizer = optim.Adam(relu_dropout_model.parameters(), lr=0.01)\n",
    "dropout_optimizer = optim.Adam(dropout_model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i, data in enumerate(loader): \n",
    "    relu_model.train()\n",
    "    relu_dropout_model.train()\n",
    "    dropout_model.train()    \n",
    "    \n",
    "    print('\\n', i+1)\n",
    "    x,y = data\n",
    "    \n",
    "    relu_optimizer.zero_grad()\n",
    "    relu_dropout_optimizer.zero_grad()\n",
    "    dropout_optimizer.zero_grad()       \n",
    "    \n",
    "    print('ReLU 모델 output \\n')\n",
    "    r_output = relu_model(x)\n",
    "    r_loss = criterion(r_output,y)\n",
    "    print(r_loss)\n",
    "    r_loss.backward()\n",
    "    relu_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU + Dropout 모델 output \\n')\n",
    "    rd_output = relu_dropout_model(x)\n",
    "    rd_loss = criterion(rd_output, y)\n",
    "    rd_loss.backward()\n",
    "    relu_dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n Dropout 모델 output \\n')\n",
    "    d_output = dropout_model(x)\n",
    "    d_loss = criterion(d_output, y)\n",
    "    d_loss.backward()\n",
    "    dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU 모델 parameters \\n')\n",
    "    for names in relu_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_model.state_dict()[names])\n",
    "        \n",
    "    print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "    for names in relu_dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_dropout_model.state_dict()[names])\n",
    "    \n",
    "    print('\\n Dropout 모델 parameters \\n')\n",
    "    for names in dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(dropout_model.state_dict()[names])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5777bc8a7577125a1c00ed1671130ea029cae56addc813f4c39a4f837e26f28b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('main': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
