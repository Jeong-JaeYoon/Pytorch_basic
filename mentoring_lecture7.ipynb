{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout과 Dead Relu는 어떤 차이점이 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_linear1 = nn.Linear(4, 3, bias=True)\n",
    "relu_linear2 = nn.Linear(3, 3, bias=True)\n",
    "relu_linear3 = nn.Linear(3, 2, bias=True)\n",
    "\n",
    "relu_drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "relu_drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "relu_drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "drop_linear1 = copy.deepcopy(relu_linear1)\n",
    "drop_linear2 = copy.deepcopy(relu_linear2)\n",
    "drop_linear3 = copy.deepcopy(relu_linear3)\n",
    "\n",
    "class Relu_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_model, self).__init__()\n",
    "        self.fc1 = relu_linear1\n",
    "        self.fc2 = relu_linear2\n",
    "        self.fc3 = relu_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Relu_Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Relu_Dropout_model, self).__init__()\n",
    "        self.fc1 = relu_drop_linear1\n",
    "        self.fc2 = relu_drop_linear2\n",
    "        self.fc3 = relu_drop_linear3\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Dropout_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dropout_model, self).__init__()\n",
    "        self.fc1 = drop_linear1\n",
    "        self.fc2 = drop_linear2\n",
    "        self.fc3 = drop_linear3\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        print(out)\n",
    "        out = self.fc3(out)\n",
    "        print(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []\n",
    "target = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.random.randint(0,10, size=4)\n",
    "    y = np.random.randint(0,2)\n",
    "    input.append(x)\n",
    "    target.append(y)\n",
    "\n",
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target)\n",
    "\n",
    "dataset = TensorDataset(input, target)\n",
    "loader = DataLoader(dataset = dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 2.]])\n",
      "tensor([0])\n",
      "tensor([[4., 9., 5., 3.]])\n",
      "tensor([1])\n",
      "tensor([[3., 9., 3., 0.]])\n",
      "tensor([1])\n",
      "tensor([[1., 9., 4., 8.]])\n",
      "tensor([0])\n",
      "tensor([[5., 8., 4., 1.]])\n",
      "tensor([1])\n",
      "tensor([[1., 1., 3., 4.]])\n",
      "tensor([0])\n",
      "tensor([[6., 1., 3., 8.]])\n",
      "tensor([1])\n",
      "tensor([[4., 4., 4., 8.]])\n",
      "tensor([0])\n",
      "tensor([[2., 6., 6., 1.]])\n",
      "tensor([1])\n",
      "tensor([[5., 6., 2., 2.]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1419,  0.4014, -0.0861,  0.2474],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4938,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1650,  0.1490],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4801,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3120,  0.2138,  0.3970],\n",
      "        [ 0.5510, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([-0.0059, -0.2946])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1419,  0.4014, -0.0861,  0.2474],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4938,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1650,  0.1490],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4801,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3120,  0.2138,  0.3970],\n",
      "        [ 0.5510, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([-0.0059, -0.2946])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1419,  0.4014, -0.0861,  0.2474],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4938,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1650,  0.1490],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4801,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3120,  0.2138,  0.3970],\n",
      "        [ 0.5510, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([-0.0059, -0.2946])\n"
     ]
    }
   ],
   "source": [
    "relu_model = Relu_model()\n",
    "relu_dropout_model = Relu_Dropout_model()\n",
    "dropout_model = Dropout_model()\n",
    "\n",
    "print('\\n ReLU 모델 parameters \\n')\n",
    "for names in relu_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_model.state_dict()[names])\n",
    "    \n",
    "print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "for names in relu_dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(relu_dropout_model.state_dict()[names])\n",
    "\n",
    "print('\\n Dropout 모델 parameters \\n')\n",
    "for names in dropout_model.state_dict():\n",
    "    print(names)\n",
    "    print(dropout_model.state_dict()[names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.3911, 0.2366]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7449, 0.6202, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.3592, -0.0875]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 2.7823, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.8783, 1.8106, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.9674, 0.1467]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.0000, 2.7823, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.8783, 0.0000, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.5802, 0.7404]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1319,  0.4114, -0.0761,  0.2574],\n",
      "        [ 0.2484, -0.1971,  0.3302, -0.4463]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5038,  0.3745])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1550,  0.1390],\n",
      "        [ 0.1499,  0.2348,  0.1268],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4701,  0.2898, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3220,  0.2238,  0.3970],\n",
      "        [ 0.5410, -0.3379,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0041, -0.3046])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1319,  0.4114, -0.0761,  0.2574],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5038,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1550,  0.1490],\n",
      "        [ 0.1499,  0.2348,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4701,  0.2898, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3220,  0.2238,  0.3970],\n",
      "        [ 0.5410, -0.3379,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0041, -0.3046])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1519,  0.3914, -0.0961,  0.2374],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4838,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1550,  0.1490],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4701,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3220,  0.2138,  0.3970],\n",
      "        [ 0.5410, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0041, -0.3046])\n",
      "\n",
      " 2\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 5.8290, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3736, 1.6585, 0.1224]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.8663, -0.0545]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[ 0.0000, 11.6579,  0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[4.5540, 0.0000, 1.5383]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.0813, 3.0054]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.0000, 10.7379, -7.2606]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0041, -0.3046]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1221,  0.4205, -0.0661,  0.2666],\n",
      "        [ 0.2551, -0.1904,  0.3369, -0.4396]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5136,  0.3812])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1457,  0.1323],\n",
      "        [ 0.1499,  0.2439,  0.1335],\n",
      "        [-0.4862,  0.1035,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4604,  0.2996, -0.5317])\n",
      "fc3.weight\n",
      "tensor([[ 0.3320,  0.2335,  0.4044],\n",
      "        [ 0.5311, -0.3476,  0.5426]])\n",
      "fc3.bias\n",
      "tensor([ 0.0139, -0.3144])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1342,  0.4050, -0.0796,  0.2512],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5015,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1469,  0.1490],\n",
      "        [ 0.1499,  0.2415,  0.1168],\n",
      "        [-0.4862,  0.1035,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4605,  0.2965, -0.5317])\n",
      "fc3.weight\n",
      "tensor([[ 0.3305,  0.2305,  0.4044],\n",
      "        [ 0.5325, -0.3446,  0.5426]])\n",
      "fc3.bias\n",
      "tensor([ 0.0136, -0.3141])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1586,  0.3847, -0.1028,  0.2307],\n",
      "        [ 0.2384, -0.2071,  0.3202, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4771,  0.3645])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1483,  0.1490],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4634,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3287,  0.2138,  0.3970],\n",
      "        [ 0.5343, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0140, -0.3145])\n",
      "\n",
      " 3\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 3.7331, 0.4437]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0631, 1.2695, 0.0315]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.6760, -0.1740]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 7.0105, 0.3529]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.6691]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.2841, 0.0489]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0.0000, 0.0000, 0.3529]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.0320, 0.0000, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3532, 0.2369]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1269,  0.4167, -0.0673,  0.2738],\n",
      "        [ 0.2630, -0.1822,  0.3434, -0.4344]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5118,  0.3877])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1466,  0.1366],\n",
      "        [ 0.1499,  0.2424,  0.1288],\n",
      "        [-0.4862,  0.1054,  0.4048]])\n",
      "fc2.bias\n",
      "tensor([ 0.4607,  0.2984, -0.5282])\n",
      "fc3.weight\n",
      "tensor([[ 0.3309,  0.2319,  0.4058],\n",
      "        [ 0.5322, -0.3459,  0.5412]])\n",
      "fc3.bias\n",
      "tensor([ 0.0129, -0.3135])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1327,  0.4016, -0.0814,  0.2464],\n",
      "        [ 0.2448, -0.2007,  0.3265, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5009,  0.3709])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1406,  0.1490],\n",
      "        [ 0.1499,  0.2467,  0.1168],\n",
      "        [-0.4862,  0.1007,  0.4048]])\n",
      "fc2.bias\n",
      "tensor([ 0.4531,  0.3017, -0.5327])\n",
      "fc3.weight\n",
      "tensor([[ 0.3371,  0.2357,  0.4078],\n",
      "        [ 0.5260, -0.3498,  0.5392]])\n",
      "fc3.bias\n",
      "tensor([ 0.0158, -0.3163])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1638,  0.3795, -0.1080,  0.2255],\n",
      "        [ 0.2448, -0.2007,  0.3265, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4720,  0.3709])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1431,  0.1554],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4636,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3303,  0.2138,  0.3970],\n",
      "        [ 0.5328, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0161, -0.3166])\n",
      "\n",
      " 4\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 3.2156, 1.1851]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0940, 1.2303, 0.2905]], grad_fn=<ReluBackward0>)\n",
      "tensor([[7.7809e-01, 3.8135e-04]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 0.0000, 1.6783]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.9955, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.2505, -0.6645]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-3.7528,  4.9652,  1.6783]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0161, -0.3166]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1340,  0.4107, -0.0722,  0.2774],\n",
      "        [ 0.2712, -0.1735,  0.3511, -0.4289]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5070,  0.3951])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1505,  0.1433],\n",
      "        [ 0.1499,  0.2380,  0.1219],\n",
      "        [-0.4862,  0.1100,  0.4122]])\n",
      "fc2.bias\n",
      "tensor([ 0.4644,  0.2940, -0.5223])\n",
      "fc3.weight\n",
      "tensor([[ 0.3264,  0.2271,  0.4004],\n",
      "        [ 0.5366, -0.3412,  0.5466]])\n",
      "fc3.bias\n",
      "tensor([ 0.0087, -0.3092])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1315,  0.3988, -0.0829,  0.2425],\n",
      "        [ 0.2412, -0.2024,  0.3235, -0.4621]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5004,  0.3687])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1354,  0.1490],\n",
      "        [ 0.1499,  0.2509,  0.1110],\n",
      "        [-0.4862,  0.0984,  0.4101]])\n",
      "fc2.bias\n",
      "tensor([ 0.4471,  0.2978, -0.5336])\n",
      "fc3.weight\n",
      "tensor([[ 0.3425,  0.2337,  0.4106],\n",
      "        [ 0.5206, -0.3478,  0.5364]])\n",
      "fc3.bias\n",
      "tensor([ 0.0138, -0.3143])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1680,  0.3752, -0.1122,  0.2213],\n",
      "        [ 0.2500, -0.1954,  0.3318, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4677,  0.3761])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1389,  0.1607],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4637,  0.2798, -0.5243])\n",
      "fc3.weight\n",
      "tensor([[ 0.3315,  0.2138,  0.3970],\n",
      "        [ 0.5316, -0.3279,  0.5501]])\n",
      "fc3.bias\n",
      "tensor([ 0.0143, -0.3148])\n",
      "\n",
      " 5\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.6768, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7167, 0.6930, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.4001, -0.1610]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.5957, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1530, -0.5215]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0., 0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.9274,  0.0000, -1.0485]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0945, -0.3986]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1394,  0.4059, -0.0746,  0.2832],\n",
      "        [ 0.2780, -0.1661,  0.3576, -0.4242]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5050,  0.4014])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1528,  0.1490],\n",
      "        [ 0.1499,  0.2351,  0.1161],\n",
      "        [-0.4862,  0.1138,  0.4183]])\n",
      "fc2.bias\n",
      "tensor([ 0.4656,  0.2921, -0.5172])\n",
      "fc3.weight\n",
      "tensor([[ 0.3239,  0.2242,  0.3958],\n",
      "        [ 0.5391, -0.3383,  0.5512]])\n",
      "fc3.bias\n",
      "tensor([ 0.0070, -0.3075])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1305,  0.3964, -0.0841,  0.2392],\n",
      "        [ 0.2381, -0.2039,  0.3210, -0.4670]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4999,  0.3669])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1311,  0.1490],\n",
      "        [ 0.1499,  0.2545,  0.1061],\n",
      "        [-0.4862,  0.0965,  0.4145]])\n",
      "fc2.bias\n",
      "tensor([ 0.4419,  0.2970, -0.5343])\n",
      "fc3.weight\n",
      "tensor([[ 0.3470,  0.2333,  0.4129],\n",
      "        [ 0.5161, -0.3473,  0.5341]])\n",
      "fc3.bias\n",
      "tensor([ 0.0136, -0.3142])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1716,  0.3717, -0.1158,  0.2177],\n",
      "        [ 0.2544, -0.1910,  0.3362, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4641,  0.3805])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1353,  0.1651],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4613,  0.2798, -0.5297])\n",
      "fc3.weight\n",
      "tensor([[ 0.3343,  0.2138,  0.3915],\n",
      "        [ 0.5288, -0.3279,  0.5555]])\n",
      "fc3.bias\n",
      "tensor([ 0.0150, -0.3155])\n",
      "\n",
      " 6\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 4.0774, 0.5336]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1683, 1.3128, 0.1701]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.7472, -0.0280]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 7.6849, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.8983, 4.5060, 0.4141]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 2.2416, -0.1624]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-4.4221,  0.0000, -0.0176]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., -0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0150, -0.3155]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1460,  0.3997, -0.0793,  0.2844],\n",
      "        [ 0.2849, -0.1594,  0.3644, -0.4179]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.5011,  0.4086])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1576,  0.1556],\n",
      "        [ 0.1499,  0.2301,  0.1095],\n",
      "        [-0.4862,  0.1197,  0.4254]])\n",
      "fc2.bias\n",
      "tensor([ 0.4693,  0.2881, -0.5106])\n",
      "fc3.weight\n",
      "tensor([[ 0.3193,  0.2193,  0.3897],\n",
      "        [ 0.5438, -0.3333,  0.5573]])\n",
      "fc3.bias\n",
      "tensor([ 0.0031, -0.3036])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1356,  0.3908, -0.0894,  0.2334],\n",
      "        [ 0.2355, -0.2051,  0.3188, -0.4712]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4950,  0.3654])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1308,  0.1490],\n",
      "        [ 0.1499,  0.2497,  0.1019],\n",
      "        [-0.4862,  0.0978,  0.4183]])\n",
      "fc2.bias\n",
      "tensor([ 0.4422,  0.2927, -0.5314])\n",
      "fc3.weight\n",
      "tensor([[ 0.3468,  0.2281,  0.4132],\n",
      "        [ 0.5163, -0.3422,  0.5338]])\n",
      "fc3.bias\n",
      "tensor([ 0.0104, -0.3110])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1747,  0.3686, -0.1189,  0.2146],\n",
      "        [ 0.2582, -0.1872,  0.3400, -0.4563]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4610,  0.3843])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1322,  0.1689],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4862,  0.1109,  0.3984]])\n",
      "fc2.bias\n",
      "tensor([ 0.4593,  0.2798, -0.5344])\n",
      "fc3.weight\n",
      "tensor([[ 0.3367,  0.2138,  0.3868],\n",
      "        [ 0.5264, -0.3279,  0.5602]])\n",
      "fc3.bias\n",
      "tensor([ 0.0131, -0.3136])\n",
      "\n",
      " 7\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 2.4158, 1.7903]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1285, 1.0402, 0.5402]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8019, 0.2644]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0.0000, 4.5311, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.0693, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.7280, 0.7574]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-1.3396,  3.6484,  2.7227]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 3.2130]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.2558, 1.4863]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1527,  0.3930, -0.0854,  0.2848],\n",
      "        [ 0.2927, -0.1517,  0.3719, -0.4110]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4962,  0.4165])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1632,  0.1631],\n",
      "        [ 0.1499,  0.2245,  0.1019],\n",
      "        [-0.4862,  0.1263,  0.4331]])\n",
      "fc2.bias\n",
      "tensor([ 0.4744,  0.2828, -0.5031])\n",
      "fc3.weight\n",
      "tensor([[ 0.3133,  0.2133,  0.3826],\n",
      "        [ 0.5497, -0.3274,  0.5644]])\n",
      "fc3.bias\n",
      "tensor([-0.0022, -0.2983])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1398,  0.3862, -0.0934,  0.2285],\n",
      "        [ 0.2333, -0.2062,  0.3169, -0.4749]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4912,  0.3640])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1314,  0.1490],\n",
      "        [ 0.1499,  0.2454,  0.0982],\n",
      "        [-0.4862,  0.0989,  0.4216]])\n",
      "fc2.bias\n",
      "tensor([ 0.4440,  0.2890, -0.5289])\n",
      "fc3.weight\n",
      "tensor([[ 0.3454,  0.2236,  0.4135],\n",
      "        [ 0.5177, -0.3376,  0.5335]])\n",
      "fc3.bias\n",
      "tensor([ 0.0062, -0.3067])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1496, -0.1928,  0.1937, -0.2502],\n",
      "        [-0.1736,  0.3707, -0.1178,  0.2129],\n",
      "        [ 0.2643, -0.1812,  0.3458, -0.4512]])\n",
      "fc1.bias\n",
      "tensor([-0.2068,  0.4604,  0.3904])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1295,  0.1722],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4912,  0.1160,  0.4035]])\n",
      "fc2.bias\n",
      "tensor([ 0.4575,  0.2798, -0.5332])\n",
      "fc3.weight\n",
      "tensor([[ 0.3388,  0.2138,  0.3807],\n",
      "        [ 0.5243, -0.3279,  0.5663]])\n",
      "fc3.bias\n",
      "tensor([ 0.0098, -0.3104])\n",
      "\n",
      " 8\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 3.3942, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0283, 1.0448, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.5429, -0.0751]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.8881, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3130, 0.1530]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-0., 0., -0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.9151, 0.5596, -0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.4395, -0.0141]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1572,  0.3878, -0.0896,  0.2880],\n",
      "        [ 0.2995, -0.1448,  0.3786, -0.4048]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4930,  0.4235])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1667,  0.1697],\n",
      "        [ 0.1499,  0.2207,  0.0952],\n",
      "        [-0.4862,  0.1321,  0.4398]])\n",
      "fc2.bias\n",
      "tensor([ 0.4775,  0.2794, -0.4964])\n",
      "fc3.weight\n",
      "tensor([[ 0.3093,  0.2092,  0.3763],\n",
      "        [ 0.5537, -0.3233,  0.5707]])\n",
      "fc3.bias\n",
      "tensor([-0.0056, -0.2950])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1434,  0.3822, -0.0969,  0.2242],\n",
      "        [ 0.2312, -0.2072,  0.3152, -0.4782]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4878,  0.3628])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1319,  0.1490],\n",
      "        [ 0.1499,  0.2417,  0.0949],\n",
      "        [-0.4862,  0.0999,  0.4246]])\n",
      "fc2.bias\n",
      "tensor([ 0.4441,  0.2857, -0.5267])\n",
      "fc3.weight\n",
      "tensor([[ 0.3447,  0.2196,  0.4137],\n",
      "        [ 0.5184, -0.3336,  0.5333]])\n",
      "fc3.bias\n",
      "tensor([ 0.0040, -0.3045])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1541, -0.1973,  0.1892, -0.2546],\n",
      "        [-0.1727,  0.3726, -0.1169,  0.2115],\n",
      "        [ 0.2696, -0.1759,  0.3508, -0.4467]])\n",
      "fc1.bias\n",
      "tensor([-0.2113,  0.4599,  0.3957])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1271,  0.1752],\n",
      "        [ 0.1499,  0.2248,  0.1168],\n",
      "        [-0.4957,  0.1205,  0.4080]])\n",
      "fc2.bias\n",
      "tensor([ 0.4543,  0.2848, -0.5321])\n",
      "fc3.weight\n",
      "tensor([[ 0.3419,  0.2188,  0.3754],\n",
      "        [ 0.5212, -0.3328,  0.5716]])\n",
      "fc3.bias\n",
      "tensor([ 0.0085, -0.3090])\n",
      "\n",
      " 9\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 2.4308, 0.9995]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0522, 0.9110, 0.2642]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6100, 0.1440]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.5713, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1294, -0.4951]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-4.5920,  4.0431,  0.9933]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 1.2427, 5.2729]], grad_fn=<MulBackward0>)\n",
      "tensor([[2.2598, 2.2914]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1625,  0.3824, -0.0939,  0.2900],\n",
      "        [ 0.3065, -0.1371,  0.3859, -0.3976]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4892,  0.4312])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1711,  0.1771],\n",
      "        [ 0.1499,  0.2162,  0.0879],\n",
      "        [-0.4862,  0.1386,  0.4474]])\n",
      "fc2.bias\n",
      "tensor([ 0.4821,  0.2747, -0.4888])\n",
      "fc3.weight\n",
      "tensor([[ 0.3042,  0.2043,  0.3693],\n",
      "        [ 0.5588, -0.3183,  0.5777]])\n",
      "fc3.bias\n",
      "tensor([-0.0102, -0.2904])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1467,  0.3787, -0.1000,  0.2204],\n",
      "        [ 0.2295, -0.2080,  0.3137, -0.4811]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4848,  0.3618])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1324,  0.1490],\n",
      "        [ 0.1499,  0.2384,  0.0920],\n",
      "        [-0.4862,  0.1008,  0.4272]])\n",
      "fc2.bias\n",
      "tensor([ 0.4442,  0.2809, -0.5247])\n",
      "fc3.weight\n",
      "tensor([[ 0.3440,  0.2156,  0.4139],\n",
      "        [ 0.5191, -0.3297,  0.5331]])\n",
      "fc3.bias\n",
      "tensor([ 0.0004, -0.3009])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1595, -0.2033,  0.1831, -0.2602],\n",
      "        [-0.1774,  0.3683, -0.1201,  0.2065],\n",
      "        [ 0.2760, -0.1703,  0.3557, -0.4410]])\n",
      "fc1.bias\n",
      "tensor([-0.2173,  0.4552,  0.4013])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1250,  0.1778],\n",
      "        [ 0.1548,  0.2199,  0.1119],\n",
      "        [-0.5014,  0.1268,  0.4136]])\n",
      "fc2.bias\n",
      "tensor([ 0.4514,  0.2836, -0.5281])\n",
      "fc3.weight\n",
      "tensor([[ 0.3447,  0.2157,  0.3688],\n",
      "        [ 0.5184, -0.3297,  0.5782]])\n",
      "fc3.bias\n",
      "tensor([ 0.0057, -0.3062])\n",
      "\n",
      " 10\n",
      "ReLU 모델 output \n",
      "\n",
      "tensor([[0.0000, 1.9350, 0.1102]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8328, 0.7027, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.3868, -0.0487]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU + Dropout 모델 output \n",
      "\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0004, -0.3009]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " Dropout 모델 output \n",
      "\n",
      "tensor([[-5.8211,  0.0000, -1.1479]], grad_fn=<MulBackward0>)\n",
      "tensor([[0., -0., 0.]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0057, -0.3062]], grad_fn=<AddmmBackward>)\n",
      "\n",
      " ReLU 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1689,  0.3773, -0.0987,  0.2882],\n",
      "        [ 0.3127, -0.1302,  0.3925, -0.3913]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4845,  0.4380])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1762,  0.1839],\n",
      "        [ 0.1499,  0.2113,  0.0812],\n",
      "        [-0.4862,  0.1445,  0.4542]])\n",
      "fc2.bias\n",
      "tensor([ 0.4878,  0.2693, -0.4821])\n",
      "fc3.weight\n",
      "tensor([[ 0.2985,  0.1989,  0.3630],\n",
      "        [ 0.5645, -0.3130,  0.5840]])\n",
      "fc3.bias\n",
      "tensor([-0.0157, -0.2848])\n",
      "\n",
      " ReLU + Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1445, -0.1877,  0.1988, -0.2451],\n",
      "        [-0.1496,  0.3755, -0.1028,  0.2169],\n",
      "        [ 0.2279, -0.2088,  0.3123, -0.4837]])\n",
      "fc1.bias\n",
      "tensor([-0.2018,  0.4822,  0.3608])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1328,  0.1490],\n",
      "        [ 0.1499,  0.2355,  0.0894],\n",
      "        [-0.4862,  0.1016,  0.4295]])\n",
      "fc2.bias\n",
      "tensor([ 0.4442,  0.2766, -0.5230])\n",
      "fc3.weight\n",
      "tensor([[ 0.3434,  0.2121,  0.4141],\n",
      "        [ 0.5196, -0.3262,  0.5329]])\n",
      "fc3.bias\n",
      "tensor([-0.0042, -0.2964])\n",
      "\n",
      " Dropout 모델 parameters \n",
      "\n",
      "fc1.weight\n",
      "tensor([[-0.1644, -0.2087,  0.1776, -0.2652],\n",
      "        [-0.1817,  0.3643, -0.1230,  0.2020],\n",
      "        [ 0.2817, -0.1653,  0.3601, -0.4359]])\n",
      "fc1.bias\n",
      "tensor([-0.2227,  0.4511,  0.4064])\n",
      "fc2.weight\n",
      "tensor([[-0.1054,  0.1231,  0.1801],\n",
      "        [ 0.1592,  0.2156,  0.1076],\n",
      "        [-0.5064,  0.1324,  0.4186]])\n",
      "fc2.bias\n",
      "tensor([ 0.4489,  0.2825, -0.5244])\n",
      "fc3.weight\n",
      "tensor([[ 0.3472,  0.2129,  0.3629],\n",
      "        [ 0.5159, -0.3269,  0.5841]])\n",
      "fc3.bias\n",
      "tensor([ 0.0016, -0.3022])\n"
     ]
    }
   ],
   "source": [
    "relu_optimizer = optim.Adam(relu_model.parameters(), lr = 0.01)\n",
    "relu_dropout_optimizer = optim.Adam(relu_dropout_model.parameters(), lr=0.01)\n",
    "dropout_optimizer = optim.Adam(dropout_model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i, data in enumerate(loader): \n",
    "    relu_model.train()\n",
    "    relu_dropout_model.train()\n",
    "    dropout_model.train()    \n",
    "    \n",
    "    print('\\n', i+1)\n",
    "    x,y = data\n",
    "    \n",
    "    relu_optimizer.zero_grad()\n",
    "    relu_dropout_optimizer.zero_grad()\n",
    "    dropout_optimizer.zero_grad()       \n",
    "    \n",
    "    print('ReLU 모델 output \\n')\n",
    "    r_output = relu_model(x)\n",
    "    r_loss = criterion(r_output,y)\n",
    "    r_loss.backward()\n",
    "    relu_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU + Dropout 모델 output \\n')\n",
    "    rd_output = relu_dropout_model(x)\n",
    "    rd_loss = criterion(rd_output, y)\n",
    "    rd_loss.backward()\n",
    "    relu_dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n Dropout 모델 output \\n')\n",
    "    d_output = dropout_model(x)\n",
    "    d_loss = criterion(d_output, y)\n",
    "    d_loss.backward()\n",
    "    dropout_optimizer.step()\n",
    "    \n",
    "    print('\\n ReLU 모델 parameters \\n')\n",
    "    for names in relu_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_model.state_dict()[names])\n",
    "        \n",
    "    print('\\n ReLU + Dropout 모델 parameters \\n')\n",
    "    for names in relu_dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(relu_dropout_model.state_dict()[names])\n",
    "    \n",
    "    print('\\n Dropout 모델 parameters \\n')\n",
    "    for names in dropout_model.state_dict():\n",
    "        print(names)\n",
    "        print(dropout_model.state_dict()[names])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5777bc8a7577125a1c00ed1671130ea029cae56addc813f4c39a4f837e26f28b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('main': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
