{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch로 시작하는 딥러닝"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 소프트맥스"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 비용함수 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x182a4670f70>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "z = torch.FloatTensor([1,2,3])\r\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "hypothesis = F.softmax(z, dim=0)\r\n",
    "print(hypothesis)\r\n",
    "print(hypothesis.sum())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# random하게 (3,5) size tensor를 정의 \r\n",
    "z = torch.rand(3, 5, requires_grad=True)\r\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
      "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
      "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "hypothesis = F.softmax(z, dim = 1)\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "hypothesis = F.softmax(z, dim = 0)\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.3412, 0.2938, 0.2671, 0.4002, 0.2469],\n",
      "        [0.3559, 0.3306, 0.3796, 0.3393, 0.3719],\n",
      "        [0.3029, 0.3756, 0.3533, 0.2605, 0.3812]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "대체로 함수에서 dim을 적는 경우가 있다. 이 경우 어떻게 해야 헷갈리지 않을까를 고민해봤다.  \r\n",
    "그 차원으로 계산을 해주세요라고 생각하면 헷갈리기 시작하므로 그냥 그 챠원은 고정하고 나머지 차원을 계산한다고 생각한다.  \r\n",
    "\r\n",
    "ex. 2차원 tensor를 가정. dim = 0이면 행은 고정하고 열에다가 연산 적용, dim = 1이면 열은 고정하고 행에다가 연산 적용."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "y = torch.randint(low=0, high=5, size=(3,)).long()\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\r\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1),1)\r\n",
    "# scatter 함수가 어려울 수 있음 아래의 사이트 보면서 항상 까먹지 말자\r\n",
    "# https://aigong.tistory.com/35"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "cost = (y_one_hot*-torch.log(hypothesis)).sum(dim=1).mean()\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0078, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "a = torch.log(F.softmax(z, dim=1))\r\n",
    "b = F.log_softmax(z, dim=1)\r\n",
    "print(a)\r\n",
    "print(b)\r\n",
    "# 결과는 동일"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
      "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
      "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)\n",
      "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
      "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
      "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "cost1 = (y_one_hot*-torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\r\n",
    "cost2 = (y_one_hot*-F.log_softmax(z, dim=1)).sum(dim=1).mean()\r\n",
    "cost3 = F.nll_loss(F.log_softmax(z, dim=1), y)\r\n",
    "cost4 = F.cross_entropy(z,y)\r\n",
    "\r\n",
    "print(cost1)\r\n",
    "print(cost2)\r\n",
    "print(cost3)\r\n",
    "print(cost4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4689, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 소프트맥스 회귀 구현 일반"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "torch.manual_seed(1)\r\n",
    "\r\n",
    "x_train = [[1, 2, 1, 1],\r\n",
    "           [2, 1, 3, 2],\r\n",
    "           [3, 1, 3, 4],\r\n",
    "           [4, 1, 5, 5],\r\n",
    "           [1, 7, 5, 5],\r\n",
    "           [1, 2, 5, 6],\r\n",
    "           [1, 6, 6, 6],\r\n",
    "           [1, 7, 7, 7]]\r\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\r\n",
    "x_train = torch.FloatTensor(x_train)\r\n",
    "y_train = torch.LongTensor(y_train)\r\n",
    "\r\n",
    "y_one_hot = torch.zeros(8,3)\r\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\r\n",
    "\r\n",
    "W = torch.zeros((4,3), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\r\n",
    "total_epoch = 1000\r\n",
    "\r\n",
    "for epoch in range(total_epoch+1):\r\n",
    "\r\n",
    "    hypothesis = F.softmax(x_train.matmul(W)+b, dim = 1)\r\n",
    "\r\n",
    "    cost = (y_one_hot*-torch.log(hypothesis)).sum(dim=1).mean()\r\n",
    "\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, total_epoch, cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "torch.manual_seed(1)\r\n",
    "\r\n",
    "x_train = [[1, 2, 1, 1],\r\n",
    "           [2, 1, 3, 2],\r\n",
    "           [3, 1, 3, 4],\r\n",
    "           [4, 1, 5, 5],\r\n",
    "           [1, 7, 5, 5],\r\n",
    "           [1, 2, 5, 6],\r\n",
    "           [1, 6, 6, 6],\r\n",
    "           [1, 7, 7, 7]]\r\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\r\n",
    "x_train = torch.FloatTensor(x_train)\r\n",
    "y_train = torch.LongTensor(y_train)\r\n",
    "\r\n",
    "y_one_hot = torch.zeros(8,3)\r\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\r\n",
    "\r\n",
    "W = torch.zeros((4,3), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\r\n",
    "\r\n",
    "total_epoch = 1000\r\n",
    "for epoch in range(total_epoch + 1):\r\n",
    "\r\n",
    "    z = x_train.matmul(W)+b\r\n",
    "    cost = F.cross_entropy(z, y_train)\r\n",
    "\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 100 == 0 :\r\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, total_epoch, cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('main': conda)"
  },
  "interpreter": {
   "hash": "5777bc8a7577125a1c00ed1671130ea029cae56addc813f4c39a4f837e26f28b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}